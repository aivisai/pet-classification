{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pet Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqsAGQzmBudA",
        "colab_type": "text"
      },
      "source": [
        "# Cats v/s Dogs Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3lW-xn5D1es",
        "colab_type": "text"
      },
      "source": [
        "### Note - Change runtime to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHpou40_C352",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXDQxGKRBDtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, MaxPool2D, Flatten, Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGEZYG60x76c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df25b06d-5b4f-40c4-f640-931025f4ec8e"
      },
      "source": [
        "print(tf.version.VERSION)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXpZpZKeC_Kt",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9fHZWK3B05S",
        "colab_type": "text"
      },
      "source": [
        "### Get the Dataset\n",
        "1. Download the Dataset from here - https://www.kaggle.com/c/dogs-vs-cats/data.  \n",
        "2. Upload it to your Google Drive and then use the following code to mount the dataset on Google Colab  \n",
        "a. !cp '/content/gdrive/My Drive/< file_path_on_google_drive >' < file_path_in_colab >"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do-L2fga__FK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "b0ec7b7c-5634-4a87-b251-e11589cbeb5b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp '/content/gdrive/My Drive/Datasets/dogs-vs-cats.zip' '/tmp/dogs-vs-cats.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLkOqxLPELKH",
        "colab_type": "text"
      },
      "source": [
        "### Unzip the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJtaz59auWaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "ac2e59e5-2333-45f9-b9ce-53eeb15d2254"
      },
      "source": [
        "zip_location = '/tmp/dogs-vs-cats.zip'\n",
        "\n",
        "with ZipFile(zip_location, 'r') as zip:\n",
        "  zip.extractall('/tmp')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-cd453664f6b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mzip_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/tmp/dogs-vs-cats.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/dogs-vs-cats.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4OTTq8AC6pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_zip_location = '/tmp/train.zip'\n",
        "test_zip_location = '/tmp/test1.zip'\n",
        "test_csv_location = '/tmp/sampleSubmission.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeXOUQTZDYH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with ZipFile(train_zip_location, 'r') as zip:\n",
        "  zip.extractall('/tmp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToH--3vuDdue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with ZipFile(test_zip_location, 'r') as zip:\n",
        "  zip.extractall('/tmp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjDaFvtIFrk_",
        "colab_type": "text"
      },
      "source": [
        "### Define Paths and Create Directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3436OWJwE_zB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = '/tmp'\n",
        "\n",
        "source_path = os.path.join(base_path, 'train') \n",
        "\n",
        "train_path = os.path.join(base_path, 'training')\n",
        "validation_path = os.path.join(base_path, 'validation')\n",
        "test_path = os.path.join(base_path, 'test1')\n",
        "\n",
        "train_cats_dir = os.path.join(train_path, 'cats')\n",
        "train_dogs_dir = os.path.join(train_path, 'dogs')\n",
        "\n",
        "validation_cats_dir = os.path.join(validation_path, 'cats')\n",
        "validation_dogs_dir = os.path.join(validation_path, 'dogs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmPs-OIuFnYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  os.mkdir(train_path)\n",
        "  os.mkdir(validation_path)\n",
        "  os.mkdir(train_cats_dir)\n",
        "  os.mkdir(train_dogs_dir)\n",
        "  os.mkdir(validation_cats_dir)\n",
        "  os.mkdir(validation_dogs_dir)\n",
        "except OSError:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p62ROtP3Hfv-",
        "colab_type": "text"
      },
      "source": [
        "### Use 90% of Files for Training and 10% for Validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZy1DtFmKgCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def copyfiles(source, list_of_files, train_dir, validation_dir, split_ratio=0.9):\n",
        "  random.shuffle(list_of_files)\n",
        "  split = int(len(list_of_files)*split_ratio)\n",
        "  train_data, validation_data = list_of_files[:split], list_of_files[split:]\n",
        "\n",
        "  for filename in train_data:\n",
        "    shutil.copyfile(os.path.join(source,filename), os.path.join(train_dir,filename))\n",
        "  \n",
        "  for filename in validation_data:\n",
        "    shutil.copyfile(os.path.join(source,filename), os.path.join(validation_dir,filename))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYF7eJbxHKw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(source, train_cats_dir, train_dogs_dir, validation_cats_dir, validation_dogs_dir, split_ratio = 0.9):\n",
        "  all_files = os.listdir(source)\n",
        "  cat_files = []\n",
        "  dog_files = []\n",
        "\n",
        "  for filename in all_files:\n",
        "    temp = filename.split('.')\n",
        "    if temp[0] == 'cat':\n",
        "      cat_files.append(filename)\n",
        "    else:\n",
        "      dog_files.append(filename)\n",
        "\n",
        "  copyfiles(source, cat_files, train_cats_dir, validation_cats_dir, split_ratio)\n",
        "  copyfiles(source, dog_files, train_dogs_dir, validation_dogs_dir, split_ratio)\n",
        "\n",
        "  return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tnj51ZnMR-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_data(source_path, train_cats_dir, train_dogs_dir, validation_cats_dir, validation_dogs_dir, 0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuWCEApzSGX3",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXt3iChMQLn4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f391ecbc-0417-4610-bdfa-168da58fc2ca"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   rotation_range = 40,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_path, batch_size = 128, class_mode = 'binary', target_size = (150,150))\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(validation_path, batch_size = 128, class_mode = 'binary', target_size = (150, 150))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 24758 images belonging to 2 classes.\n",
            "Found 4758 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSMC06af4E7B",
        "colab_type": "text"
      },
      "source": [
        " ## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzBGE0W9u5q8",
        "colab_type": "text"
      },
      "source": [
        "Using Classes helps when you want to customize your models and it's layers - https://www.tensorflow.org/guide/keras/custom_layers_and_models \n",
        "\n",
        "If you want to Improve accuracy further you can try the following\n",
        "1. Increase Epochs\n",
        "2. Add Dropouts\n",
        "3. Add l2 regularization\n",
        "4. try adding more layers\n",
        "5. Add Batch Norm\n",
        "6. Use a PreTrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNZT_zW34RKx",
        "colab_type": "text"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pZLMxXHW7bG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(Model):\n",
        "  def __init__(self):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.conv1 = Conv2D(16, (5,5), activation='relu', padding = 'same')\n",
        "    self.maxpool = MaxPool2D(2,2)\n",
        "    self.conv2 = Conv2D(32, (5,5), activation='relu', padding = 'same')\n",
        "    self.conv3 = Conv2D(64, (3,3), activation='relu', padding = 'same')\n",
        "    self.conv4 = Conv2D(128, (3,3), activation='relu', padding = 'same')\n",
        "    self.flatten = Flatten()\n",
        "    self.dense1 = Dense(512, activation='relu')\n",
        "    self.dense2 = Dense(1, activation='sigmoid')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.dense2(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lthCdMgXYVrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = Classifier()\n",
        "\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIPd18WbsOdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "7271b9ec-f6c1-4b6d-d90e-74f4e90487f0"
      },
      "source": [
        "classifier.build(input_shape = (128, 150, 150, 3))\n",
        "classifier.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"classifier\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              multiple                  1216      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) multiple                  0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            multiple                  12832     \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            multiple                  18496     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            multiple                  73856     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  5308928   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  513       \n",
            "=================================================================\n",
            "Total params: 5,415,841\n",
            "Trainable params: 5,415,841\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3nSmFdf4Wwg",
        "colab_type": "text"
      },
      "source": [
        "### Define Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0BZG8vPyWVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save Checkpoints in your Drive if you want to restart training for a particular epoch\n",
        "# Include the epoch in the file name (uses `str.format`)\n",
        "checkpoint_path = \"/content/gdrive/My Drive/Checkpoints/cp{epoch:04d}.ckpt\" \n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "try:\n",
        "  os.mkdir(checkpoint_dir)\n",
        "except OSError:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOA0YoljeCsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a callback that saves the model's weights every epoch and keeps the best weights after completion of training\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    verbose=1, \n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    save_freq='epoch')\n",
        "\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 3, restore_best_weights=True, min_delta = 0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JutWdDqzeAgw",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baAS6M_5YXm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "outputId": "07484a25-0dc8-4202-f72d-c51983f3fecb"
      },
      "source": [
        "history = classifier.fit(train_generator, validation_data=validation_generator, epochs = 10, callbacks = [early_stop_callback, cp_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "194/194 [==============================] - ETA: 0s - loss: 0.6840 - accuracy: 0.5656\n",
            "Epoch 00001: val_loss improved from inf to 0.65249, saving model to /content/gdrive/My Drive/Checkpoints/cp0001.ckpt\n",
            "194/194 [==============================] - 184s 947ms/step - loss: 0.6840 - accuracy: 0.5656 - val_loss: 0.6525 - val_accuracy: 0.5801\n",
            "Epoch 2/10\n",
            "194/194 [==============================] - ETA: 0s - loss: 0.6213 - accuracy: 0.6504\n",
            "Epoch 00002: val_loss improved from 0.65249 to 0.57254, saving model to /content/gdrive/My Drive/Checkpoints/cp0002.ckpt\n",
            "194/194 [==============================] - 183s 942ms/step - loss: 0.6213 - accuracy: 0.6504 - val_loss: 0.5725 - val_accuracy: 0.6908\n",
            "Epoch 3/10\n",
            "194/194 [==============================] - ETA: 0s - loss: 0.5870 - accuracy: 0.6844\n",
            "Epoch 00003: val_loss improved from 0.57254 to 0.51552, saving model to /content/gdrive/My Drive/Checkpoints/cp0003.ckpt\n",
            "194/194 [==============================] - 183s 945ms/step - loss: 0.5870 - accuracy: 0.6844 - val_loss: 0.5155 - val_accuracy: 0.7430\n",
            "Epoch 4/10\n",
            "194/194 [==============================] - ETA: 0s - loss: 0.5564 - accuracy: 0.7128\n",
            "Epoch 00004: val_loss improved from 0.51552 to 0.48309, saving model to /content/gdrive/My Drive/Checkpoints/cp0004.ckpt\n",
            "194/194 [==============================] - 183s 942ms/step - loss: 0.5564 - accuracy: 0.7128 - val_loss: 0.4831 - val_accuracy: 0.7699\n",
            "Epoch 5/10\n",
            "194/194 [==============================] - ETA: 0s - loss: 0.5195 - accuracy: 0.7421\n",
            "Epoch 00005: val_loss did not improve from 0.48309\n",
            "194/194 [==============================] - 182s 936ms/step - loss: 0.5195 - accuracy: 0.7421 - val_loss: 0.5037 - val_accuracy: 0.7505\n",
            "Epoch 6/10\n",
            "194/194 [==============================] - ETA: 0s - loss: 0.4966 - accuracy: 0.7586\n",
            "Epoch 00006: val_loss improved from 0.48309 to 0.40554, saving model to /content/gdrive/My Drive/Checkpoints/cp0006.ckpt\n",
            "194/194 [==============================] - 183s 945ms/step - loss: 0.4966 - accuracy: 0.7586 - val_loss: 0.4055 - val_accuracy: 0.8134\n",
            "Epoch 7/10\n",
            "194/194 [==============================] - ETA: 0s - loss: 0.4775 - accuracy: 0.7707\n",
            "Epoch 00007: val_loss improved from 0.40554 to 0.38269, saving model to /content/gdrive/My Drive/Checkpoints/cp0007.ckpt\n",
            "194/194 [==============================] - 183s 941ms/step - loss: 0.4775 - accuracy: 0.7707 - val_loss: 0.3827 - val_accuracy: 0.8306\n",
            "Epoch 8/10\n",
            "194/194 [==============================] - ETA: 0s - loss: 0.4521 - accuracy: 0.7862\n",
            "Epoch 00008: val_loss improved from 0.38269 to 0.35027, saving model to /content/gdrive/My Drive/Checkpoints/cp0008.ckpt\n",
            "194/194 [==============================] - 184s 951ms/step - loss: 0.4521 - accuracy: 0.7862 - val_loss: 0.3503 - val_accuracy: 0.8491\n",
            "Epoch 9/10\n",
            "194/194 [==============================] - ETA: 0s - loss: 0.4210 - accuracy: 0.8083\n",
            "Epoch 00009: val_loss did not improve from 0.35027\n",
            "194/194 [==============================] - 185s 951ms/step - loss: 0.4210 - accuracy: 0.8083 - val_loss: 0.3689 - val_accuracy: 0.8350\n",
            "Epoch 10/10\n",
            "194/194 [==============================] - ETA: 0s - loss: 0.4175 - accuracy: 0.8066\n",
            "Epoch 00010: val_loss improved from 0.35027 to 0.32532, saving model to /content/gdrive/My Drive/Checkpoints/cp0010.ckpt\n",
            "194/194 [==============================] - 184s 951ms/step - loss: 0.4175 - accuracy: 0.8066 - val_loss: 0.3253 - val_accuracy: 0.8541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPjpD5vbYjIm",
        "colab_type": "text"
      },
      "source": [
        "### Freeze Model\n",
        "We need to Freeze the model before deploying on OpenVino"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY5rLZ8BYpzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in classifier.layers:\n",
        "  layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFhvqMzmeY7M",
        "colab_type": "text"
      },
      "source": [
        "### Save Model\n",
        "This will save the model's architecture, weights and training configuration. This allows you to export a model o it can be used without access to the original Python code.\n",
        "\n",
        "Saving a fully-functional model is useful as you can load them in TensorFlow.js, run on mobile devices using TF Lite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3Wgu580ZnUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save Model in your Drive if you want to restart training for a particular epoch\n",
        "model_path = \"/content/gdrive/My Drive/Models/pet_classification\" \n",
        "model_dir = os.path.dirname(model_path)\n",
        "\n",
        "try:\n",
        "  os.mkdir(model_dir)\n",
        "except OSError:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDJZdGLn34t9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "698116af-b6ee-483b-b83d-f32dfa3a1fb3"
      },
      "source": [
        "classifier.save(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Models/pet_classification/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DHX3S9feo07",
        "colab_type": "text"
      },
      "source": [
        "To Load the saved model you can use the following code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be8v7MNvZgze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = tf.keras.models.load_model(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rUR2FRY3pIC",
        "colab_type": "text"
      },
      "source": [
        "## To Do\n",
        "1. Add more comments\n",
        "2. Try Improving val accuracy to 95%\n",
        "3. Plots\n",
        "4. (New) add keras tuner\n",
        "5. Add TensorBoard "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYhsleAwi1r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path1 =  \"/content/gdrive/My Drive/images/c2.jpg\"\n",
        "\n",
        "from keras.preprocessing import image\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvspmG3fN-Du",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "7742eeb6-9d1a-4a03-e9c6-235dc507caff"
      },
      "source": [
        "from IPython.display import Image \n",
        "\n",
        "Image(path1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4kUG1MFLyVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img1 = image.load_img(path1, target_size=(150, 150))\n",
        "img_arr =  image.img_to_array(img1)\n",
        "x = np.expand_dims(img_arr, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ltFBygiJg7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = classifier.predict(x)[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIrKZF3qKxeB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1417a5ec-6b22-4001-f688-be4f7a7ec84d"
      },
      "source": [
        "if int(y) == 1:\n",
        "  print(\"dog\")\n",
        "else:\n",
        "  print(\"cat\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgsyWMRvTbHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}