{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pet Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNXmByfn9f0Vw+LFQTTA7+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aivis-ai/pet-classification/blob/master/Pet_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqsAGQzmBudA",
        "colab_type": "text"
      },
      "source": [
        "# Cats v/s Dogs Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3lW-xn5D1es",
        "colab_type": "text"
      },
      "source": [
        "### Note - Change runtime to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHpou40_C352",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXDQxGKRBDtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, MaxPool2D, Flatten, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGEZYG60x76c",
        "colab_type": "code",
        "outputId": "4dee6e99-a3d5-47e7-df0c-770cd63f6a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tf.version.VERSION)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXpZpZKeC_Kt",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9fHZWK3B05S",
        "colab_type": "text"
      },
      "source": [
        "### Get the Dataset\n",
        "1. Download the Dataset from here - https://www.kaggle.com/c/dogs-vs-cats/data.  \n",
        "2. Upload it to your Google Drive and then use the following code to mount the dataset on Google Colab  \n",
        "a. !cp '/content/gdrive/My Drive/< file_path_on_google_drive >' < file_path_in_colab >"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do-L2fga__FK",
        "colab_type": "code",
        "outputId": "a6621c6b-c25f-4bc9-eae2-8b24b988285c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp '/content/gdrive/My Drive/Datasets/dogs-vs-cats.zip' '/tmp/dogs-vs-cats.zip'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLkOqxLPELKH",
        "colab_type": "text"
      },
      "source": [
        "### Unzip the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJtaz59auWaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_location = '/tmp/dogs-vs-cats.zip'\n",
        "\n",
        "with ZipFile(zip_location, 'r') as zip:\n",
        "  zip.extractall('/tmp')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4OTTq8AC6pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_zip_location = '/tmp/train.zip'\n",
        "test_zip_location = '/tmp/test1.zip'\n",
        "test_csv_location = '/tmp/sampleSubmission.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeXOUQTZDYH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with ZipFile(train_zip_location, 'r') as zip:\n",
        "  zip.extractall('/tmp')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToH--3vuDdue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with ZipFile(test_zip_location, 'r') as zip:\n",
        "  zip.extractall('/tmp')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjDaFvtIFrk_",
        "colab_type": "text"
      },
      "source": [
        "### Define Paths and Create Directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3436OWJwE_zB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = '/tmp'\n",
        "\n",
        "source_path = os.path.join(base_path, 'train') \n",
        "\n",
        "train_path = os.path.join(base_path, 'training')\n",
        "validation_path = os.path.join(base_path, 'validation')\n",
        "test_path = os.path.join(base_path, 'test1')\n",
        "\n",
        "train_cats_dir = os.path.join(train_path, 'cats')\n",
        "train_dogs_dir = os.path.join(train_path, 'dogs')\n",
        "\n",
        "validation_cats_dir = os.path.join(validation_path, 'cats')\n",
        "validation_dogs_dir = os.path.join(validation_path, 'dogs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmPs-OIuFnYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  os.mkdir(train_path)\n",
        "  os.mkdir(validation_path)\n",
        "  os.mkdir(train_cats_dir)\n",
        "  os.mkdir(train_dogs_dir)\n",
        "  os.mkdir(validation_cats_dir)\n",
        "  os.mkdir(validation_dogs_dir)\n",
        "except OSError:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p62ROtP3Hfv-",
        "colab_type": "text"
      },
      "source": [
        "### Use 90% of Files for Training and 10% for Validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZy1DtFmKgCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def copyfiles(source, list_of_files, train_dir, validation_dir, split_ratio=0.9):\n",
        "  random.shuffle(list_of_files)\n",
        "  split = int(len(list_of_files)*split_ratio)\n",
        "  train_data, validation_data = list_of_files[:split], list_of_files[split:]\n",
        "\n",
        "  for filename in train_data:\n",
        "    shutil.copyfile(os.path.join(source,filename), os.path.join(train_dir,filename))\n",
        "  \n",
        "  for filename in validation_data:\n",
        "    shutil.copyfile(os.path.join(source,filename), os.path.join(validation_dir,filename))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYF7eJbxHKw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(source, train_cats_dir, train_dogs_dir, validation_cats_dir, validation_dogs_dir, split_ratio = 0.9):\n",
        "  all_files = os.listdir(source)\n",
        "  cat_files = []\n",
        "  dog_files = []\n",
        "\n",
        "  for filename in all_files:\n",
        "    temp = filename.split('.')\n",
        "    if temp[0] == 'cat':\n",
        "      cat_files.append(filename)\n",
        "    else:\n",
        "      dog_files.append(filename)\n",
        "\n",
        "  copyfiles(source, cat_files, train_cats_dir, validation_cats_dir, split_ratio)\n",
        "  copyfiles(source, dog_files, train_dogs_dir, validation_dogs_dir, split_ratio)\n",
        "\n",
        "  return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tnj51ZnMR-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_data(source_path, train_cats_dir, train_dogs_dir, validation_cats_dir, validation_dogs_dir, 0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuWCEApzSGX3",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXt3iChMQLn4",
        "colab_type": "code",
        "outputId": "f50b8e68-deac-4934-9e46-70a9a56c281e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   rotation_range = 40,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_path, batch_size = 128, class_mode = 'binary', target_size = (150,150))\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(validation_path, batch_size = 128, class_mode = 'binary', target_size = (150, 150))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 22500 images belonging to 2 classes.\n",
            "Found 2500 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSMC06af4E7B",
        "colab_type": "text"
      },
      "source": [
        " ## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzBGE0W9u5q8",
        "colab_type": "text"
      },
      "source": [
        "Using Classes helps when you want to customize your models and it's layers - https://www.tensorflow.org/guide/keras/custom_layers_and_models \n",
        "\n",
        "If you want to Improve accuracy further you can try the following\n",
        "1. Increase Epochs\n",
        "2. Add Dropouts\n",
        "3. Add l2 regularization\n",
        "4. try adding more layers\n",
        "5. Add Batch Norm\n",
        "6. Use a PreTrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNZT_zW34RKx",
        "colab_type": "text"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pZLMxXHW7bG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(Model):\n",
        "  def __init__(self):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.conv1 = Conv2D(16, (5,5), activation='relu', padding = 'same')\n",
        "    self.maxpool = MaxPool2D(2,2)\n",
        "    self.conv2 = Conv2D(32, (5,5), activation='relu', padding = 'same')\n",
        "    self.conv3 = Conv2D(64, (3,3), activation='relu', padding = 'same')\n",
        "    self.conv4 = Conv2D(128, (3,3), activation='relu', padding = 'same')\n",
        "    self.flatten = Flatten()\n",
        "    self.dense1 = Dense(512, activation='relu')\n",
        "    self.dense2 = Dense(1, activation='sigmoid')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.dense2(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lthCdMgXYVrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = Classifier()\n",
        "\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIPd18WbsOdB",
        "colab_type": "code",
        "outputId": "467d1f9c-0981-4dec-e55d-e4c73b5b44e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "classifier.build(input_shape = (128, 150, 150, 3))\n",
        "classifier.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"classifier\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              multiple                  1216      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) multiple                  0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            multiple                  12832     \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            multiple                  18496     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            multiple                  73856     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  5308928   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  513       \n",
            "=================================================================\n",
            "Total params: 5,415,841\n",
            "Trainable params: 5,415,841\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3nSmFdf4Wwg",
        "colab_type": "text"
      },
      "source": [
        "### Define Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0BZG8vPyWVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save Checkpoints in your Drive if you want to restart training for a particular epoch\n",
        "# Include the epoch in the file name (uses `str.format`)\n",
        "checkpoint_path = \"/content/gdrive/My Drive/Checkpoints/cp{epoch:04d}.ckpt\" \n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "try:\n",
        "  os.mkdir(checkpoint_dir)\n",
        "except OSError:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOA0YoljeCsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a callback that saves the model's weights every epoch and keeps the best weights after completion of training\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    verbose=1, \n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    save_freq='epoch')\n",
        "\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 3, restore_best_weights=True, min_delta = 0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JutWdDqzeAgw",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baAS6M_5YXm-",
        "colab_type": "code",
        "outputId": "47b3fee9-44e3-4024-92ee-7b2f618636cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "history = classifier.fit(train_generator, validation_data=validation_generator, epochs = 10, callbacks = [early_stop_callback, cp_callback])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.6948 - accuracy: 0.5136\n",
            "Epoch 00001: val_loss improved from inf to 0.68281, saving model to /content/gdrive/My Drive/Checkpoints/cp0001.ckpt\n",
            "176/176 [==============================] - 184s 1s/step - loss: 0.6948 - accuracy: 0.5136 - val_loss: 0.6828 - val_accuracy: 0.5452\n",
            "Epoch 2/10\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.6606 - accuracy: 0.6040\n",
            "Epoch 00002: val_loss improved from 0.68281 to 0.63359, saving model to /content/gdrive/My Drive/Checkpoints/cp0002.ckpt\n",
            "176/176 [==============================] - 185s 1s/step - loss: 0.6606 - accuracy: 0.6040 - val_loss: 0.6336 - val_accuracy: 0.6352\n",
            "Epoch 3/10\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.6249 - accuracy: 0.6453\n",
            "Epoch 00003: val_loss improved from 0.63359 to 0.59518, saving model to /content/gdrive/My Drive/Checkpoints/cp0003.ckpt\n",
            "176/176 [==============================] - 185s 1s/step - loss: 0.6249 - accuracy: 0.6453 - val_loss: 0.5952 - val_accuracy: 0.6764\n",
            "Epoch 4/10\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.5939 - accuracy: 0.6798\n",
            "Epoch 00004: val_loss improved from 0.59518 to 0.52922, saving model to /content/gdrive/My Drive/Checkpoints/cp0004.ckpt\n",
            "176/176 [==============================] - 183s 1s/step - loss: 0.5939 - accuracy: 0.6798 - val_loss: 0.5292 - val_accuracy: 0.7440\n",
            "Epoch 5/10\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.5474 - accuracy: 0.7196\n",
            "Epoch 00005: val_loss improved from 0.52922 to 0.47029, saving model to /content/gdrive/My Drive/Checkpoints/cp0005.ckpt\n",
            "176/176 [==============================] - 183s 1s/step - loss: 0.5474 - accuracy: 0.7196 - val_loss: 0.4703 - val_accuracy: 0.7788\n",
            "Epoch 6/10\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.5165 - accuracy: 0.7456\n",
            "Epoch 00006: val_loss did not improve from 0.47029\n",
            "176/176 [==============================] - 181s 1s/step - loss: 0.5165 - accuracy: 0.7456 - val_loss: 0.4753 - val_accuracy: 0.7796\n",
            "Epoch 7/10\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.4971 - accuracy: 0.7543\n",
            "Epoch 00007: val_loss improved from 0.47029 to 0.43420, saving model to /content/gdrive/My Drive/Checkpoints/cp0007.ckpt\n",
            "176/176 [==============================] - 181s 1s/step - loss: 0.4971 - accuracy: 0.7543 - val_loss: 0.4342 - val_accuracy: 0.8044\n",
            "Epoch 8/10\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.4756 - accuracy: 0.7720\n",
            "Epoch 00008: val_loss improved from 0.43420 to 0.40666, saving model to /content/gdrive/My Drive/Checkpoints/cp0008.ckpt\n",
            "176/176 [==============================] - 182s 1s/step - loss: 0.4756 - accuracy: 0.7720 - val_loss: 0.4067 - val_accuracy: 0.8124\n",
            "Epoch 9/10\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.4478 - accuracy: 0.7902\n",
            "Epoch 00009: val_loss improved from 0.40666 to 0.40081, saving model to /content/gdrive/My Drive/Checkpoints/cp0009.ckpt\n",
            "176/176 [==============================] - 182s 1s/step - loss: 0.4478 - accuracy: 0.7902 - val_loss: 0.4008 - val_accuracy: 0.8216\n",
            "Epoch 10/10\n",
            "176/176 [==============================] - ETA: 0s - loss: 0.4355 - accuracy: 0.7964\n",
            "Epoch 00010: val_loss improved from 0.40081 to 0.37763, saving model to /content/gdrive/My Drive/Checkpoints/cp0010.ckpt\n",
            "176/176 [==============================] - 180s 1s/step - loss: 0.4355 - accuracy: 0.7964 - val_loss: 0.3776 - val_accuracy: 0.8244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPjpD5vbYjIm",
        "colab_type": "text"
      },
      "source": [
        "### Freeze Model\n",
        "We need to Freeze the model before deploying on OpenVino"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY5rLZ8BYpzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in classifier.layers:\n",
        "  layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFhvqMzmeY7M",
        "colab_type": "text"
      },
      "source": [
        "### Save Model\n",
        "This will save the model's architecture, weights and training configuration. This allows you to export a model o it can be used without access to the original Python code.\n",
        "\n",
        "Saving a fully-functional model is useful as you can load them in TensorFlow.js, run on mobile devices using TF Lite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3Wgu580ZnUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save Model in your Drive if you want to restart training for a particular epoch\n",
        "model_path = \"/content/gdrive/My Drive/Models/pet_classification\" \n",
        "model_dir = os.path.dirname(model_path)\n",
        "\n",
        "try:\n",
        "  os.mkdir(model_dir)\n",
        "except OSError:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDJZdGLn34t9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "7546356e-ec7a-4914-ec19-14a9f31cb8c6"
      },
      "source": [
        "classifier.save(model_path)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Models/pet_classification/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DHX3S9feo07",
        "colab_type": "text"
      },
      "source": [
        "To Load the saved model you can use the following code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be8v7MNvZgze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = tf.keras.models.load_model(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs7AHDTvSPLf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "3b9f8acd-5934-472f-9c8e-63f1c61da749"
      },
      "source": [
        "!pip install git+https://github.com/onnx/tensorflow-onnx"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/onnx/tensorflow-onnx\n",
            "  Cloning https://github.com/onnx/tensorflow-onnx to /tmp/pip-req-build-ynlq3l0g\n",
            "  Running command git clone -q https://github.com/onnx/tensorflow-onnx /tmp/pip-req-build-ynlq3l0g\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.6/dist-packages (from tf2onnx==1.7.0) (1.18.5)\n",
            "Collecting onnx>=1.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/ee/bc7bc88fc8449266add978627e90c363069211584b937fd867b0ccc59f09/onnx-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tf2onnx==1.7.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tf2onnx==1.7.0) (1.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx>=1.4.1->tf2onnx==1.7.0) (3.6.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx>=1.4.1->tf2onnx==1.7.0) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tf2onnx==1.7.0) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tf2onnx==1.7.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tf2onnx==1.7.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tf2onnx==1.7.0) (2020.4.5.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx>=1.4.1->tf2onnx==1.7.0) (47.1.1)\n",
            "Building wheels for collected packages: tf2onnx\n",
            "  Building wheel for tf2onnx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tf2onnx: filename=tf2onnx-1.7.0-cp36-none-any.whl size=181185 sha256=58643f86a4667e6eeb70cda6b3b46361fe72ed45ae5f15620060209c55620cda\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1_34hwg0/wheels/db/db/21/74f30455028095a1ee011391af71fb68fde8660aad68602f2a\n",
            "Successfully built tf2onnx\n",
            "Installing collected packages: onnx, tf2onnx\n",
            "Successfully installed onnx-1.7.0 tf2onnx-1.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni04YXewangh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d488a47e-44b0-4056-f4e8-18a0e5bc477a"
      },
      "source": [
        "!python -m tf2onnx.convert --saved-model '/content/gdrive/My Drive/Models/pet_classification' --output '/content/gdrive/My Drive/Models/model.onnx'"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-13 22:17:31.645842: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-13 22:17:33.739935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-06-13 22:17:33.742879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:33.743771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-06-13 22:17:33.743830: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-13 22:17:33.745802: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-06-13 22:17:33.756504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-06-13 22:17:33.756936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-06-13 22:17:33.765569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-06-13 22:17:33.766672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-06-13 22:17:33.771413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-13 22:17:33.771570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:33.772368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:33.773081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
            "2020-06-13 22:17:33.778997: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz\n",
            "2020-06-13 22:17:33.779324: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x54aa1c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-06-13 22:17:33.779377: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-06-13 22:17:33.888145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:33.889012: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x54aa380 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-06-13 22:17:33.889077: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-06-13 22:17:33.889304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:33.890027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-06-13 22:17:33.890081: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-13 22:17:33.890131: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-06-13 22:17:33.890167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-06-13 22:17:33.890202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-06-13 22:17:33.890244: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-06-13 22:17:33.890287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-06-13 22:17:33.890335: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-13 22:17:33.890451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:33.891259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:33.891952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
            "2020-06-13 22:17:33.892008: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-13 22:17:34.476380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-06-13 22:17:34.476452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
            "2020-06-13 22:17:34.476476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
            "2020-06-13 22:17:34.476742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:34.477418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:34.478125: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-06-13 22:17:34.478179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10615 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "2020-06-13 22:17:34.825201: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 21233664 exceeds 10% of free system memory.\n",
            "2020-06-13 22:17:34.883353: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 21233664 exceeds 10% of free system memory.\n",
            "2020-06-13 22:17:34.960575: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 21233664 exceeds 10% of free system memory.\n",
            "2020-06-13 22:17:35.165531: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 21233664 exceeds 10% of free system memory.\n",
            "2020-06-13 22:17:35.209129: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 21233664 exceeds 10% of free system memory.\n",
            "2020-06-13 22:17:35.385896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.386853: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
            "2020-06-13 22:17:35.387062: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
            "2020-06-13 22:17:35.387771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.388556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-06-13 22:17:35.388636: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-13 22:17:35.388718: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-06-13 22:17:35.388830: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-06-13 22:17:35.388895: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-06-13 22:17:35.388930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-06-13 22:17:35.388969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-06-13 22:17:35.389003: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-13 22:17:35.389152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.390004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.390759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
            "2020-06-13 22:17:35.390812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-06-13 22:17:35.390837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
            "2020-06-13 22:17:35.390857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
            "2020-06-13 22:17:35.391022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.391813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.392482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10615 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "2020-06-13 22:17:35.397073: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\n",
            "2020-06-13 22:17:35.397123: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 73 nodes (58), 103 edges (88), time = 1.764ms.\n",
            "2020-06-13 22:17:35.397139: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.067ms.\n",
            "2020-06-13 22:17:35.738436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.739426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-06-13 22:17:35.739511: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-13 22:17:35.739572: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-06-13 22:17:35.739613: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-06-13 22:17:35.739652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-06-13 22:17:35.739691: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-06-13 22:17:35.739744: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-06-13 22:17:35.739782: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-13 22:17:35.739896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.740611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.741257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
            "2020-06-13 22:17:35.741308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-06-13 22:17:35.741341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
            "2020-06-13 22:17:35.741355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
            "2020-06-13 22:17:35.741478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.742169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.742916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10615 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf2onnx/tf_loader.py:324: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "2020-06-13 22:17:35,803 - WARNING - From /usr/local/lib/python3.6/dist-packages/tf2onnx/tf_loader.py:324: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "2020-06-13 22:17:35.807053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.807855: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
            "2020-06-13 22:17:35.808103: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
            "2020-06-13 22:17:35.808860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.809596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-06-13 22:17:35.809654: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-13 22:17:35.809710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-06-13 22:17:35.809768: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-06-13 22:17:35.809813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-06-13 22:17:35.809848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-06-13 22:17:35.809909: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-06-13 22:17:35.809986: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-13 22:17:35.810152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.810995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.811669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
            "2020-06-13 22:17:35.811718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-06-13 22:17:35.811759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
            "2020-06-13 22:17:35.811780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
            "2020-06-13 22:17:35.811907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.812703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:35.813417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10615 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "2020-06-13 22:17:36.056876: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\n",
            "2020-06-13 22:17:36.056985: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 61 nodes (-12), 79 edges (-24), time = 183.503ms.\n",
            "2020-06-13 22:17:36.057003: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.15ms.\n",
            "2020-06-13 22:17:36.057034: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 61 nodes (0), 79 edges (0), time = 30.371ms.\n",
            "2020-06-13 22:17:36.057050: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.539ms.\n",
            "2020-06-13 22:17:36.200549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:36.201284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-06-13 22:17:36.201336: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-13 22:17:36.201407: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-06-13 22:17:36.201451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-06-13 22:17:36.201489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-06-13 22:17:36.201527: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-06-13 22:17:36.201561: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-06-13 22:17:36.201594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-13 22:17:36.201724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:36.202440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:36.203085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
            "2020-06-13 22:17:36.203136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-06-13 22:17:36.203159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
            "2020-06-13 22:17:36.203173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
            "2020-06-13 22:17:36.203313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:36.204090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-13 22:17:36.204694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10615 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "2020-06-13 22:17:36,204 - INFO - Using tensorflow=2.2.0, onnx=1.7.0, tf2onnx=1.7.0/3bf9ea\n",
            "2020-06-13 22:17:36,205 - INFO - Using opset <onnx, 8>\n",
            "2020-06-13 22:17:37,708 - INFO - Optimizing ONNX model\n",
            "2020-06-13 22:17:37,757 - INFO - After optimization: Identity -5 (5->0), Transpose -14 (16->2)\n",
            "2020-06-13 22:17:37,759 - INFO - \n",
            "2020-06-13 22:17:37,759 - INFO - Successfully converted TensorFlow model /content/gdrive/My Drive/Models/pet_classification to ONNX\n",
            "2020-06-13 22:17:38,387 - INFO - ONNX model is saved at /content/gdrive/My Drive/Models/model.onnx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9lux6y4a4b_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "\n",
        "\n",
        "def freeze(model, outputdir):\n",
        "# Convert Keras model to ConcreteFunction\n",
        "    full_model = tf.function(lambda x: model(x))\n",
        "    full_model = full_model.get_concrete_function(\n",
        "        tf.TensorSpec((1, 150, 150, 3), tf.float32)\n",
        "    )\n",
        "    frozen_func = convert_variables_to_constants_v2(full_model)\n",
        "    frozen_func.graph.as_graph_def()\n",
        "    # Save frozen graph from frozen ConcreteFunction to hard drive\n",
        "    path = tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n",
        "                      logdir=outputdir,\n",
        "                      name=\"frozen_graph.pb\",\n",
        "                      as_text=False)\n",
        "\n",
        "    print(path)\n",
        "    layers = [op.name for op in frozen_func.graph.get_operations()]\n",
        "\n",
        "    print(\"Frozen model layers: \")\n",
        "    for layerName in layers:\n",
        "        print(f\"layer: {layerName}\")\n",
        " \n",
        "    print(\"-\" * 50)\n",
        "    print(\"Frozen model inputs: \")\n",
        "    print(frozen_func.inputs)\n",
        "    print(\"Frozen model outputs: \")\n",
        "    print(frozen_func.outputs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe3DNNTfiomM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4824d0ba-1188-410e-e47c-7a367bc88e39"
      },
      "source": [
        "freeze(classifier, './content/gdrive/My Drive/Models/')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./content/gdrive/My Drive/Models/frozen_graph.pb\n",
            "Frozen model layers: \n",
            "layer: x\n",
            "layer: classifier/9244\n",
            "layer: classifier/9230\n",
            "layer: classifier/9242\n",
            "layer: classifier/9252\n",
            "layer: classifier/9232\n",
            "layer: classifier/9246\n",
            "layer: classifier/9238\n",
            "layer: classifier/9234\n",
            "layer: classifier/9250\n",
            "layer: classifier/9240\n",
            "layer: classifier/9236\n",
            "layer: classifier/9248\n",
            "layer: Func/classifier/StatefulPartitionedCall/input_control_node/_0\n",
            "layer: Func/classifier/StatefulPartitionedCall/input/_1\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/input_control_node/_16\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/input/_17\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d/StatefulPartitionedCall/input_control_node/_32\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d/StatefulPartitionedCall/input/_33\n",
            "layer: Func/classifier/StatefulPartitionedCall/input/_2\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/input/_18\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d/StatefulPartitionedCall/input/_34\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d/StatefulPartitionedCall/Conv2D/ReadVariableOp\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d/StatefulPartitionedCall/Conv2D\n",
            "layer: Func/classifier/StatefulPartitionedCall/input/_3\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/input/_19\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d/StatefulPartitionedCall/input/_35\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d/StatefulPartitionedCall/BiasAdd/ReadVariableOp\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d/StatefulPartitionedCall/BiasAdd\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d/StatefulPartitionedCall/Relu\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d/StatefulPartitionedCall/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d/StatefulPartitionedCall/output/_36\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d/StatefulPartitionedCall/output_control_node/_37\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d/PartitionedCall/input_control_node/_38\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d/PartitionedCall/input/_39\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d/PartitionedCall/MaxPool\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d/PartitionedCall/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d/PartitionedCall/output/_40\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_1/StatefulPartitionedCall/input_control_node/_41\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_1/StatefulPartitionedCall/input/_42\n",
            "layer: Func/classifier/StatefulPartitionedCall/input/_4\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/input/_20\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_1/StatefulPartitionedCall/input/_43\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_1/StatefulPartitionedCall/Conv2D/ReadVariableOp\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_1/StatefulPartitionedCall/Conv2D\n",
            "layer: Func/classifier/StatefulPartitionedCall/input/_5\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/input/_21\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_1/StatefulPartitionedCall/input/_44\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_1/StatefulPartitionedCall/BiasAdd/ReadVariableOp\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_1/StatefulPartitionedCall/BiasAdd\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_1/StatefulPartitionedCall/Relu\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_1/StatefulPartitionedCall/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_1/StatefulPartitionedCall/output/_45\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_1/StatefulPartitionedCall/output_control_node/_46\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_1/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_1/PartitionedCall/input_control_node/_47\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_1/PartitionedCall/input/_48\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_1/PartitionedCall/MaxPool\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_1/PartitionedCall/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_1/PartitionedCall/output/_49\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_1/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_2/StatefulPartitionedCall/input_control_node/_50\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_2/StatefulPartitionedCall/input/_51\n",
            "layer: Func/classifier/StatefulPartitionedCall/input/_6\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/input/_22\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_2/StatefulPartitionedCall/input/_52\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_2/StatefulPartitionedCall/Conv2D/ReadVariableOp\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_2/StatefulPartitionedCall/Conv2D\n",
            "layer: Func/classifier/StatefulPartitionedCall/input/_7\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/input/_23\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_2/StatefulPartitionedCall/input/_53\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_2/StatefulPartitionedCall/BiasAdd/ReadVariableOp\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_2/StatefulPartitionedCall/BiasAdd\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_2/StatefulPartitionedCall/Relu\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_2/StatefulPartitionedCall/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_2/StatefulPartitionedCall/output/_54\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_2/StatefulPartitionedCall/output_control_node/_55\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_2/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_2/PartitionedCall/input_control_node/_56\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_2/PartitionedCall/input/_57\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_2/PartitionedCall/MaxPool\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_2/PartitionedCall/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_2/PartitionedCall/output/_58\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_2/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_3/StatefulPartitionedCall/input_control_node/_59\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_3/StatefulPartitionedCall/input/_60\n",
            "layer: Func/classifier/StatefulPartitionedCall/input/_8\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/input/_24\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_3/StatefulPartitionedCall/input/_61\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_3/StatefulPartitionedCall/Conv2D/ReadVariableOp\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_3/StatefulPartitionedCall/Conv2D\n",
            "layer: Func/classifier/StatefulPartitionedCall/input/_9\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/input/_25\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_3/StatefulPartitionedCall/input/_62\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_3/StatefulPartitionedCall/BiasAdd/ReadVariableOp\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_3/StatefulPartitionedCall/BiasAdd\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_3/StatefulPartitionedCall/Relu\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_3/StatefulPartitionedCall/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_3/StatefulPartitionedCall/output/_63\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_3/StatefulPartitionedCall/output_control_node/_64\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/conv2d_3/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_3/PartitionedCall/input_control_node/_65\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_3/PartitionedCall/input/_66\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_3/PartitionedCall/MaxPool\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_3/PartitionedCall/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_3/PartitionedCall/output/_67\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/max_pooling2d_3/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/flatten/PartitionedCall/input_control_node/_68\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/flatten/PartitionedCall/input/_69\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/flatten/PartitionedCall/Shape\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/flatten/PartitionedCall/strided_slice/stack\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/flatten/PartitionedCall/strided_slice/stack_1\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/flatten/PartitionedCall/strided_slice/stack_2\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/flatten/PartitionedCall/strided_slice\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/flatten/PartitionedCall/Reshape/shape/1\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/flatten/PartitionedCall/Reshape/shape\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/flatten/PartitionedCall/Reshape\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/flatten/PartitionedCall/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/flatten/PartitionedCall/output/_70\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/flatten/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense/StatefulPartitionedCall/input_control_node/_71\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense/StatefulPartitionedCall/input/_72\n",
            "layer: Func/classifier/StatefulPartitionedCall/input/_10\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/input/_26\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense/StatefulPartitionedCall/input/_73\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense/StatefulPartitionedCall/MatMul/ReadVariableOp\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense/StatefulPartitionedCall/MatMul\n",
            "layer: Func/classifier/StatefulPartitionedCall/input/_11\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/input/_27\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense/StatefulPartitionedCall/input/_74\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense/StatefulPartitionedCall/BiasAdd/ReadVariableOp\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense/StatefulPartitionedCall/BiasAdd\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense/StatefulPartitionedCall/Relu\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense/StatefulPartitionedCall/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense/StatefulPartitionedCall/output/_75\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense/StatefulPartitionedCall/output_control_node/_76\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense_1/StatefulPartitionedCall/input_control_node/_77\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense_1/StatefulPartitionedCall/input/_78\n",
            "layer: Func/classifier/StatefulPartitionedCall/input/_12\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/input/_28\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense_1/StatefulPartitionedCall/input/_79\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense_1/StatefulPartitionedCall/MatMul/ReadVariableOp\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense_1/StatefulPartitionedCall/MatMul\n",
            "layer: Func/classifier/StatefulPartitionedCall/input/_13\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/input/_29\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense_1/StatefulPartitionedCall/input/_80\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense_1/StatefulPartitionedCall/BiasAdd/ReadVariableOp\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense_1/StatefulPartitionedCall/BiasAdd\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense_1/StatefulPartitionedCall/Sigmoid\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense_1/StatefulPartitionedCall/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense_1/StatefulPartitionedCall/output/_81\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense_1/StatefulPartitionedCall/output_control_node/_82\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/dense_1/Identity\n",
            "layer: classifier/StatefulPartitionedCall/StatefulPartitionedCall/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/output/_30\n",
            "layer: Func/classifier/StatefulPartitionedCall/StatefulPartitionedCall/output_control_node/_31\n",
            "layer: classifier/StatefulPartitionedCall/Identity\n",
            "layer: Func/classifier/StatefulPartitionedCall/output/_14\n",
            "layer: Func/classifier/StatefulPartitionedCall/output_control_node/_15\n",
            "layer: Identity\n",
            "--------------------------------------------------\n",
            "Frozen model inputs: \n",
            "[<tf.Tensor 'x:0' shape=(1, 150, 150, 3) dtype=float32>]\n",
            "Frozen model outputs: \n",
            "[<tf.Tensor 'Identity:0' shape=(1, 1) dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rUR2FRY3pIC",
        "colab_type": "text"
      },
      "source": [
        "## To Do\n",
        "1. Add more comments\n",
        "2. Try Improving val accuracy to 95%\n",
        "3. Compare with testing data\n",
        "4. Plots\n",
        "5. (New) add keras tuner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYhsleAwi1r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}